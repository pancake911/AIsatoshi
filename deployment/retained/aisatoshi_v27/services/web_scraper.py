"""
AIsatoshi V27 - ç½‘é¡µæµè§ˆæœåŠ¡

æ”¯æŒï¼š
1. APIä¼˜å…ˆè·å–æ•°æ®ï¼ˆCoinGeckoç­‰ï¼‰
2. HTMLè§£æ
3. å­é¡µé¢æ·±åº¦æµè§ˆ
"""

import re
import json
import time
from typing import Dict, List, Optional, Any
from urllib.parse import urljoin, urlparse
import requests
from bs4 import BeautifulSoup
from core.logger import Logger
from core.exceptions import WebScrapingError


class WebScraper:
    """ç½‘é¡µæµè§ˆæœåŠ¡

    æ”¯æŒå¤šç§æµè§ˆæ–¹å¼ï¼š
    - APIä¼˜å…ˆï¼ˆå¿«é€Ÿï¼‰
    - HTMLè§£æï¼ˆé€šç”¨ï¼‰
    - æ·±åº¦æµè§ˆï¼ˆè®¿é—®å­é¡µé¢ï¼‰
    """

    def __init__(self, logger: Optional[Logger] = None):
        """åˆå§‹åŒ–ç½‘é¡µæµè§ˆæœåŠ¡

        Args:
            logger: æ—¥å¿—è®°å½•å™¨
        """
        self.logger = logger or Logger(name="WebScraper")
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

        # å·²è®¿é—®çš„URLï¼ˆé¿å…é‡å¤ï¼‰
        self.visited_urls = set()

        # APIé…ç½®
        self.apis = {
            'coingecko': {
                'base_url': 'https://api.coingecko.com/api/v3',
                'endpoints': {
                    'price': '/simple/price',
                    'market_cap': '/global',
                }
            }
        }

        self.logger.info("ç½‘é¡µæµè§ˆæœåŠ¡å·²åˆå§‹åŒ–")

    # === APIä¼˜å…ˆæ–¹æ³• ===

    def get_crypto_price(self, coin_id: str) -> Dict[str, Any]:
        """è·å–åŠ å¯†è´§å¸ä»·æ ¼ï¼ˆAPIä¼˜å…ˆï¼‰

        Args:
            coin_id: ä»£å¸IDï¼ˆå¦‚bitcoin, ethereumï¼‰

        Returns:
            ä»·æ ¼ä¿¡æ¯
        """
        try:
            url = f"{self.apis['coingecko']['base_url']}/simple/price"
            params = {
                'ids': coin_id,
                'vs_currencies': 'usd,cny',
                'include_market_cap': 'true',
                'include_24hr_change': 'true'
            }

            response = self.session.get(url, params=params, timeout=10)
            response.raise_for_status()

            data = response.json()

            if coin_id in data:
                return data[coin_id]
            else:
                return {}

        except Exception as e:
            self.logger.error(f"è·å–ä»·æ ¼å¤±è´¥: {e}")
            return {}

    def get_global_market_data(self) -> Dict[str, Any]:
        """è·å–å…¨çƒå¸‚åœºæ•°æ®

        Returns:
            å¸‚åœºæ•°æ®
        """
        try:
            url = f"{self.apis['coingecko']['base_url']}/global"
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.json().get('data', {})

        except Exception as e:
            self.logger.error(f"è·å–å¸‚åœºæ•°æ®å¤±è´¥: {e}")
            return {}

    # === HTMLè§£ææ–¹æ³• ===

    def browse(self, url: str, depth: int = 1) -> Dict[str, Any]:
        """æµè§ˆç½‘é¡µï¼ˆæ”¯æŒå­é¡µé¢ï¼‰

        Args:
            url: ç›®æ ‡URL
            depth: æ·±åº¦ï¼ˆ0=ä»…ä¸»é¡µï¼Œ1=ä¸»é¡µ+ä¸€çº§é“¾æ¥ï¼‰

        Returns:
            æµè§ˆç»“æœ
        """
        self.logger.info(f"æµè§ˆç½‘é¡µ: {url}, æ·±åº¦: {depth}")

        result = {
            'url': url,
            'title': '',
            'content': '',
            'links': [],
            'sub_pages': [],
            'error': None
        }

        try:
            # è·å–ä¸»é¡µé¢
            response = self.session.get(url, timeout=15)
            response.raise_for_status()

            # è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')

            # æå–æ ‡é¢˜
            result['title'] = self._extract_title(soup)

            # æå–ä¸»è¦å†…å®¹
            result['content'] = self._extract_content(soup)

            # æå–é“¾æ¥
            links = self._extract_links(url, soup)
            result['links'] = links[:20]  # é™åˆ¶é“¾æ¥æ•°é‡

            # æ·±åº¦æµè§ˆï¼šè®¿é—®ç›¸å…³å­é¡µé¢
            if depth > 0 and links:
                sub_pages = self._browse_sub_pages(links[:3], depth-1)  # æœ€å¤š3ä¸ªå­é¡µé¢
                result['sub_pages'] = sub_pages

        except Exception as e:
            result['error'] = str(e)
            self.logger.error(f"æµè§ˆå¤±è´¥: {e}")

        return result

    def _extract_title(self, soup: BeautifulSoup) -> str:
        """æå–é¡µé¢æ ‡é¢˜"""
        # ä¼˜å…ˆçº§ï¼šh1 > title > meta og:title
        if soup.h1:
            return soup.h1.get_text(strip=True)
        if soup.title:
            return soup.title.get_text(strip=True)
        meta_og_title = soup.find('meta', property='og:title')
        if meta_og_title:
            return meta_og_title.get('content', '')
        return "æ— æ ‡é¢˜"

    def _extract_content(self, soup: BeautifulSoup) -> str:
        """æå–é¡µé¢ä¸»è¦å†…å®¹"""
        # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            tag.decompose()

        # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content = (
            soup.find('main') or
            soup.find('article') or
            soup.find('div', class_=re.compile(r'content|main|article', re.I)) or
            soup.body
        )

        if main_content:
            # æå–æ–‡æœ¬ï¼Œä¿ç•™æ®µè½ç»“æ„
            paragraphs = []
            for p in main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):
                text = p.get_text(strip=True)
                if text and len(text) > 10:  # å¿½ç•¥å¤ªçŸ­çš„å†…å®¹
                    paragraphs.append(text)

            return '\n\n'.join(paragraphs[:30])  # æœ€å¤š30æ®µ

        return soup.get_text(separator='\n', strip=True)[:2000]

    def _extract_links(self, base_url: str, soup: BeautifulSoup) -> List[Dict]:
        """æå–é¡µé¢é“¾æ¥"""
        links = []
        base_domain = urlparse(base_url).netloc

        for a in soup.find_all('a', href=True):
            href = a['href']
            text = a.get_text(strip=True)[:50]

            # è½¬æ¢ä¸ºç»å¯¹URL
            absolute_url = urljoin(base_url, href)

            # åªä¿ç•™åŒåŸŸåçš„é“¾æ¥ï¼ˆé¿å…çˆ¬å–å¤–éƒ¨ç½‘ç«™ï¼‰
            if urlparse(absolute_url).netloc == base_domain:
                links.append({
                    'url': absolute_url,
                    'text': text
                })

        return links

    def _browse_sub_pages(self, links: List[Dict], depth: int) -> List[Dict]:
        """æµè§ˆå­é¡µé¢"""
        sub_pages = []

        for link in links:
            if depth <= 0:
                break

            url = link['url']

            # é¿å…é‡å¤è®¿é—®
            if url in self.visited_urls:
                continue
            self.visited_urls.add(url)

            # è®¿é—®å­é¡µé¢
            try:
                response = self.session.get(url, timeout=10)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, 'html.parser')

                sub_pages.append({
                    'url': url,
                    'title': self._extract_title(soup),
                    'content': self._extract_content(soup)[:500],  # å­é¡µé¢åªå–æ‘˜è¦
                })

                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

            except Exception as e:
                self.logger.debug(f"å­é¡µé¢æµè§ˆå¤±è´¥ {url}: {e}")

        return sub_pages

    # === æ™ºèƒ½æµè§ˆ ===

    def smart_browse(self, url: str, question: str = "") -> str:
        """æ™ºèƒ½æµè§ˆï¼šæ ¹æ®URLç±»å‹é€‰æ‹©æœ€ä½³æ–¹å¼

        Args:
            url: ç›®æ ‡URL
            question: ç”¨æˆ·é—®é¢˜ï¼ˆç”¨äºèšç„¦å†…å®¹ï¼‰

        Returns:
            æ ¼å¼åŒ–çš„æµè§ˆç»“æœ
        """
        self.logger.info(f"æ™ºèƒ½æµè§ˆ: {url}")

        # æ£€æµ‹URLç±»å‹
        url_type = self._detect_url_type(url)

        if url_type == 'coingecko_price':
            # CoinGeckoä»·æ ¼é¡µé¢
            coin_id = self._extract_coin_id(url)
            if coin_id:
                price_data = self.get_crypto_price(coin_id)
                return self._format_price_data(coin_id, price_data)

        elif url_type == 'coingecko':
            # CoinGeckoå…¶ä»–é¡µé¢
            return self._browse_coingecko(url)

        # é€šç”¨æµè§ˆ
        result = self.browse(url, depth=1)

        return self._format_browse_result(result, question)

    def _detect_url_type(self, url: str) -> str:
        """æ£€æµ‹URLç±»å‹"""
        if 'coingecko.com' in url:
            if '/price/' in url or '/en/coins/' in url:
                return 'coingecko_price'
            return 'coingecko'
        return 'general'

    def _extract_coin_id(self, url: str) -> Optional[str]:
        """ä»CoinGecko URLæå–ä»£å¸ID"""
        # ä¾‹å¦‚ï¼šhttps://www.coingecko.com/en/coins/bitcoin
        match = re.search(r'/en/coins/([^/]+)', url)
        if match:
            return match.group(1)
        return None

    def _browse_coingecko(self, url: str) -> str:
        """æµè§ˆCoinGeckoé¡µé¢"""
        result = self.browse(url, depth=0)
        return self._format_browse_result(result)

    def _format_price_data(self, coin_id: str, data: Dict) -> str:
        """æ ¼å¼åŒ–ä»·æ ¼æ•°æ®"""
        if not data:
            return f"âŒ æ— æ³•è·å– {coin_id} çš„ä»·æ ¼æ•°æ®"

        lines = [f"ğŸ’° {coin_id.upper()} ä»·æ ¼ä¿¡æ¯"]

        if 'usd' in data:
            lines.append(f"\nğŸ’µ ä»·æ ¼: ${data['usd']:,.2f} USD")

        if 'cny' in data:
            lines.append(f"ğŸ’´ ä»·æ ¼: Â¥{data['cny']:,.2f} CNY")

        if 'usd_market_cap' in data:
            lines.append(f"ğŸ“Š å¸‚å€¼: ${data['usd_market_cap']:,.0f}")

        if 'usd_24h_change' in data:
            change = data['usd_24h_change']
            emoji = "ğŸ“ˆ" if change > 0 else "ğŸ“‰"
            lines.append(f"{emoji} 24h: {change:+.2f}%")

        return '\n'.join(lines)

    def _format_browse_result(self, result: Dict, question: str = "") -> str:
        """æ ¼å¼åŒ–æµè§ˆç»“æœ"""
        if result.get('error'):
            return f"âŒ æµè§ˆå¤±è´¥: {result['error']}"

        lines = [f"ğŸ“„ {result['title']}", f"ğŸ”— {result['url']}"]

        # ä¸»è¦å†…å®¹
        content = result['content']
        if question:
            # å¦‚æœæœ‰é—®é¢˜ï¼Œå°è¯•æå–ç›¸å…³éƒ¨åˆ†
            lines.append(f"\nã€ç›¸å…³å†…å®¹ã€‘")
            lines.append(content[:1000])
        else:
            lines.append(f"\nã€å†…å®¹æ‘˜è¦ã€‘")
            lines.append(content[:1500])

        # å­é¡µé¢
        if result.get('sub_pages'):
            lines.append(f"\nã€ç›¸å…³é¡µé¢ã€‘")
            for page in result['sub_pages'][:3]:
                lines.append(f"- {page['title']}")
                lines.append(f"  {page['content'][:100]}...")

        return '\n'.join(lines)

    # === æ‰¹é‡æµè§ˆ ===

    def browse_multiple(self, urls: List[str]) -> List[Dict]:
        """æ‰¹é‡æµè§ˆå¤šä¸ªURL

        Args:
            urls: URLåˆ—è¡¨

        Returns:
            æµè§ˆç»“æœåˆ—è¡¨
        """
        results = []

        for url in urls:
            try:
                result = self.browse(url, depth=0)
                results.append(result)
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

            except Exception as e:
                results.append({
                    'url': url,
                    'error': str(e)
                })

        return results

    # === å·¥å…·æ–¹æ³• ===

    def is_valid_url(self, url: str) -> bool:
        """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except:
            return False

    def clean_text(self, html: str) -> str:
        """æ¸…ç†HTMLæ–‡æœ¬"""
        soup = BeautifulSoup(html, 'html.parser')
        return soup.get_text(separator='\n', strip=True)
